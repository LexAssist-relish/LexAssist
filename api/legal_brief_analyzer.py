import os
import json
import logging
import nltk
import re
import spacy
from typing import Dict, List, Any, Optional, Union
from .indian_kanoon import IndianKanoonAPI

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('legal_brief_analyzer')

# Download necessary NLTK data
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

# Load spaCy model
try:
    nlp = spacy.load("en_core_web_sm")
except OSError:
    # If model not found, download it
    os.system("python -m spacy download en_core_web_sm")
    nlp = spacy.load("en_core_web_sm")

class LegalBriefAnalyzer:
    """
    Analyzes legal briefs to extract key information and generate search queries
    for the Indian Kanoon API.
    """
    
    def __init__(self, api_key: str):
        """
        Initialize the Legal Brief Analyzer.
        
        Args:
            api_key: The API key for Indian Kanoon
        """
        self.api_client = IndianKanoonAPI(api_key)
        self.stopwords = set(nltk.corpus.stopwords.words('english'))
        # Add legal stopwords
        self.stopwords.update(['court', 'case', 'plaintiff', 'defendant', 'petitioner', 'respondent'])
        logger.info("Legal Brief Analyzer initialized")
    
    def analyze_brief(self, brief_text: str) -> Dict[str, Any]:
        """
        Analyze a legal brief and extract relevant information.
        
        Args:
            brief_text: The text of the legal brief
            
        Returns:
            Dict containing extracted information and search results
        """
        logger.info("Analyzing legal brief")
        
        # Extract key entities and concepts
        entities = self._extract_entities(brief_text)
        
        # Extract legal acts and sections
        acts_sections = self._extract_acts_sections(brief_text)
        
        # Extract case citations
        citations = self._extract_citations(brief_text)
        
        # Generate search queries
        queries = self._generate_search_queries(brief_text, entities, acts_sections, citations)
        
        # Search for relevant law sections
        law_sections = self._search_law_sections(queries, acts_sections)
        
        # Search for relevant case histories
        case_histories = self._search_case_histories(queries, citations)
        
        # Generate legal analysis
        analysis = self._generate_analysis(brief_text, law_sections, case_histories)
        
        return {
            "entities": entities,
            "acts_sections": acts_sections,
            "citations": citations,
            "law_sections": law_sections,
            "case_histories": case_histories,
            "analysis": analysis
        }
    
    def _extract_entities(self, text: str) -> Dict[str, List[str]]:
        """
        Extract named entities from the brief text.
        
        Args:
            text: The brief text
            
        Returns:
            Dict of entity types and their values
        """
        doc = nlp(text)
        entities = {}
        
        for ent in doc.ents:
            if ent.label_ not in entities:
                entities[ent.label_] = []
            if ent.text not in entities[ent.label_]:
                entities[ent.label_].append(ent.text)
        
        # Extract legal entities using regex patterns
        legal_entities = {
            "PERSON": [],
            "ORG": [],
            "DATE": [],
            "LOCATION": []
        }
        
        # Merge spaCy entities with regex-extracted entities
        for key in legal_entities:
            if key in entities:
                legal_entities[key].extend(entities[key])
        
        return legal_entities
    
    def _extract_acts_sections(self, text: str) -> List[Dict[str, str]]:
        """
        Extract references to legal acts and sections.
        
        Args:
            text: The brief text
            
        Returns:
            List of dicts containing act names and section numbers
        """
        # Common Indian legal acts
        common_acts = [
            "Indian Penal Code", "IPC", 
            "Code of Criminal Procedure", "CrPC",
            "Code of Civil Procedure", "CPC",
            "Indian Contract Act",
            "Indian Evidence Act",
            "Constitution of India",
            "Income Tax Act",
            "Companies Act",
            "Specific Relief Act",
            "Arbitration and Conciliation Act",
            "Consumer Protection Act"
        ]
        
        acts_sections = []
        
        # Pattern for "Act name + Section + number"
        act_section_pattern = r'(?i)({})\s*,?\s*(?:section|sec\.|s\.|ยง)\s*(\d+(?:\([a-zA-Z0-9]\))?(?:-\d+)?)'
        
        # Search for each act
        for act in common_acts:
            pattern = act_section_pattern.format(re.escape(act))
            matches = re.finditer(pattern, text)
            
            for match in matches:
                acts_sections.append({
                    "act": match.group(1),
                    "section": match.group(2)
                })
        
        # Pattern for "Section + number + of + Act name"
        section_act_pattern = r'(?i)(?:section|sec\.|s\.|ยง)\s*(\d+(?:\([a-zA-Z0-9]\))?(?:-\d+)?)\s*of\s*the\s*({})'
        
        # Search for each act
        for act in common_acts:
            pattern = section_act_pattern.format(re.escape(act))
            matches = re.finditer(pattern, text)
            
            for match in matches:
                acts_sections.append({
                    "act": match.group(2),
                    "section": match.group(1)
                })
        
        return acts_sections
    
    def _extract_citations(self, text: str) -> List[Dict[str, str]]:
        """
        Extract case citations from the brief text.
        
        Args:
            text: The brief text
            
        Returns:
            List of dicts containing citation information
        """
        citations = []
        
        # Pattern for AIR citations
        air_pattern = r'(\d{4})\s*AIR\s*(\d+)'
        air_matches = re.finditer(air_pattern, text)
        for match in air_matches:
            citations.append({
                "type": "AIR",
                "year": match.group(1),
                "page": match.group(2)
            })
        
        # Pattern for SCC citations
        scc_pattern = r'\((\d{4})\)\s*(\d+)\s*SCC\s*(\d+)'
        scc_matches = re.finditer(scc_pattern, text)
        for match in scc_matches:
            citations.append({
                "type": "SCC",
                "year": match.group(1),
                "volume": match.group(2),
                "page": match.group(3)
            })
        
        # Pattern for SCR citations
        scr_pattern = r'(\d{4})\s*SCR\s*(\d+)'
        scr_matches = re.finditer(scr_pattern, text)
        for match in scr_matches:
            citations.append({
                "type": "SCR",
                "year": match.group(1),
                "page": match.group(2)
            })
        
        return citations
    
    def _generate_search_queries(self, text: str, entities: Dict[str, List[str]], 
                               acts_sections: List[Dict[str, str]], 
                               citations: List[Dict[str, str]]) -> List[str]:
        """
        Generate search queries based on the brief analysis.
        
        Args:
            text: The brief text
            entities: Extracted entities
            acts_sections: Extracted acts and sections
            citations: Extracted citations
            
        Returns:
            List of search queries
        """
        queries = []
        
        # Tokenize and clean the text
        tokens = nltk.word_tokenize(text.lower())
        tokens = [token for token in tokens if token.isalnum() and token not in self.stopwords]
        
        # Extract key phrases using TextRank-like algorithm
        doc = nlp(text)
        key_phrases = []
        for chunk in doc.noun_chunks:
            if len(chunk.text.split()) >= 2 and len(chunk.text.split()) <= 5:
                key_phrases.append(chunk.text)
        
        # Add key phrases to queries
        for phrase in key_phrases[:5]:  # Limit to top 5 phrases
            queries.append(phrase)
        
        # Add act and section specific queries
        for item in acts_sections:
            queries.append(f"{item['act']} section {item['section']}")
        
        # Add citation specific queries
        for item in citations:
            if item['type'] == 'AIR':
                queries.append(f"{item['year']} AIR {item['page']}")
            elif item['type'] == 'SCC':
                queries.append(f"({item['year']}) {item['volume']} SCC {item['page']}")
            elif item['type'] == 'SCR':
                queries.append(f"{item['year']} SCR {item['page']}")
        
        # Add entity-based queries
        for entity_type, values in entities.items():
            if entity_type in ['PERSON', 'ORG'] and values:
                for value in values[:3]:  # Limit to top 3 entities per type
                    # Combine with key legal terms from the brief
                    legal_terms = self._extract_legal_terms(text)
                    if legal_terms:
                        queries.append(f"{value} {legal_terms[0]}")
        
        # Remove duplicates and limit to reasonable number
        unique_queries = list(set(queries))
        return unique_queries[:10]  # Limit to top 10 queries
    
    def _extract_legal_terms(self, text: str) -> List[str]:
        """
        Extract common legal terms from the text.
        
        Args:
            text: The brief text
            
        Returns:
            List of legal terms
        """
        legal_terms = [
            "murder", "theft", "fraud", "negligence", "damages", 
            "contract", "breach", "tort", "liability", "compensation",
            "injunction", "specific performance", "arbitration", "appeal",
            "evidence", "testimony", "witness", "jurisdiction", "bail",
            "conviction", "acquittal", "sentence", "punishment", "rights"
        ]
        
        found_terms = []
        for term in legal_terms:
            if re.search(r'\b' + re.escape(term) + r'\b', text, re.IGNORECASE):
                found_terms.append(term)
        
        return found_terms
    
    def _search_law_sections(self, queries: List[str], 
                           acts_sections: List[Dict[str, str]]) -> List[Dict[str, Any]]:
        """
        Search for relevant law sections using the generated queries.
        
        Args:
            queries: List of search queries
            acts_sections: Extracted acts and sections
            
        Returns:
            List of relevant law sections
        """
        law_sections = []
        
        # First, search for specific acts and sections
        for item in acts_sections:
            query = f"{item['act']} section {item['section']}"
            try:
                results = self.api_client.search(query, doc_types="laws", max_cites=5)
                
                if 'docs' in results and results['docs']:
                    for doc in results['docs'][:3]:  # Limit to top 3 results per query
                        # Get full document to extract the section content
                        if 'tid' in doc:
                            doc_details = self.api_client.get_document(doc['tid'])
                            if 'doc' in doc_details:
                                law_sections.append({
                                    "title": item['act'],
                                    "sectionNumber": item['section'],
                                    "content": self._extract_clean_content(doc_details['doc']),
                                    "relevance": 9,  # High relevance for exact matches
                                    "source": "Indian Kanoon",
                                    "docId": doc['tid']
                                })
            except Exception as e:
                logger.error(f"Error searching for law section: {str(e)}")
        
        # Then, search for general legal concepts
        for query in queries:
            if not any(query == f"{item['act']} section {item['section']}" for item in acts_sections):
                try:
                    results = self.api_client.search(query, doc_types="laws", max_cites=5)
                    
                    if 'docs' in results and results['docs']:
                        for doc in results['docs'][:2]:  # Limit to top 2 results per query
                            # Check if this document is already included
                            if not any(section.get('docId') == doc.get('tid') for section in law_sections):
                                # Get full document
                                if 'tid' in doc:
                                    doc_details = self.api_client.get_document(doc['tid'])
                                    if 'doc' in doc_details:
                                        # Extract act and section from title
                                        act_section = self._parse_act_section_from_title(doc.get('title', ''))
                                        
                                        law_sections.append({
                                            "title": act_section.get('act', doc.get('title', 'Unknown Act')),
                                            "sectionNumber": act_section.get('section', 'N/A'),
                                            "content": self._extract_clean_content(doc_details['doc']),
                                            "relevance": 7,  # Medium relevance for concept matches
                                            "source": "Indian Kanoon",
                                            "docId": doc['tid']
                                        })
                except Exception as e:
                    logger.error(f"Error searching for law section: {str(e)}")
        
        # Sort by relevance
        law_sections.sort(key=lambda x: x['relevance'], reverse=True)
        
        return law_sections
    
    def _search_case_histories(self, queries: List[str], 
                             citations: List[Dict[str, str]]) -> List[Dict[str, Any]]:
        """
        Search for relevant case histories using the generated queries.
        
        Args:
            queries: List of search queries
            citations: Extracted citations
            
        Returns:
            List of relevant case histories
        """
        case_histories = []
        
        # First, search for specific citations
        for item in citations:
            citation_str = ""
            if item['type'] == 'AIR':
                citation_str = f"{item['year']} AIR {item['page']}"
            elif item['type'] == 'SCC':
                citation_str = f"({item['year']}) {item['volume']} SCC {item['page']}"
            elif item['type'] == 'SCR':
                citation_str = f"{item['year']} SCR {item['page']}"
            
            try:
                results = self.api_client.search(citation_str, doc_types="judgments", max_cites=5)
                
                if 'docs' in results and results['docs']:
                    for doc in results['docs'][:2]:  # Limit to top 2 results per citation
                        # Get full document to extract the case details
                        if 'tid' in doc:
                            doc_details = self.api_client.get_document(doc['tid'])
                            if 'doc' in doc_details:
                                # Extract parties from title
                                parties = doc.get('title', 'Unknown Parties')
                                
                                case_histories.append({
                                    "citation": citation_str,
                                    "parties": parties,
                                    "holdings": self._extract_holdings(doc_details['doc']),
                                    "relevance": 9,  # High relevance for exact citation matches
                                    "date": self._extract_date(doc_details['doc']),
                                    "source": "Indian Kanoon",
                                    "docId": doc['tid']
                                })
            except Exception as e:
                logger.error(f"Error searching for case history: {str(e)}")
        
        # Then, search for general legal concepts in case law
        for query in queries:
            try:
                results = self.api_client.search(query, doc_types="judgments", max_cites=5)
                
                if 'docs' in results and results['docs']:
                    for doc in results['docs'][:2]:  # Limit to top 2 results per query
                        # Check if this document is already included
                        if not any(case.get('docId') == doc.get('tid') for case in case_histories):
                            # Get full document
                            if 'tid' in doc:
                                doc_details = self.api_client.get_document(doc['tid'])
                                if 'doc' in doc_details:
                                    # Extract citation from metadata
                                    citation = self._extract_citation(doc_details['doc'])
                                    
                                    case_histories.append({
                                        "citation": citation if citation else doc.get('docsource', 'Unknown Citation'),
                                        "parties": doc.get('title', 'Unknown Parties'),
                                        "holdings": self._extract_holdings(doc_details['doc']),
                                        "relevance": 7,  # Medium relevance for concept matches
                                        "date": self._extract_date(doc_details['doc']),
                                        "source": "Indian Kanoon",
                                        "docId": doc['tid']
                                    })
            except Exception as e:
                logger.error(f"Error searching for case history: {str(e)}")
        
        # Sort by relevance
        case_histories.sort(key=lambda x: x['relevance'], reverse=True)
        
        return case_histories
    
    def _parse_act_section_from_title(self, title: str) -> Dict[str, str]:
        """
        Parse act and section information from a document title.
        
        Args:
            title: The document title
            
        Returns:
            Dict containing act and section information
        """
        result = {"act": "", "section": ""}
        
        # Pattern for "Act name, Section number"
        act_section_pattern = r'(.*?),?\s*(?:Section|Sec\.|S\.|ยง)\s*(\d+(?:\([a-zA-Z0-9]\))?(?:-\d+)?)'
        match = re.search(act_section_pattern, title, re.IGNORECASE)
        
        if match:
            result["act"] = match.group(1).strip()
            result["section"] = match.group(2)
        else:
            result["act"] = title
        
        return result
    
    def _extract_clean_content(self, html_content: str) -> str:
        """
        Extract clean text content from HTML.
        
        Args:
            html_content: HTML content
            
        Returns:
            Clean text content
        """
        # Simple HTML tag removal (for more complex cases, use BeautifulSoup)
        text = re.sub(r'<[^>]+>', ' ', html_content)
        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        # Limit to reasonable length
        if len(text) > 1000:
            text = text[:997] + "..."
        
        return text
    
    def _extract_holdings(self, html_content: str) -> str:
        """
        Extract the holdings (main decision) from a case document.
        
        Args:
            html_content: HTML content of the case document
            
        Returns:
            Extracted holdings
        """
        # Clean HTML
        text = self._extract_clean_content(html_content)
        
        # Look for sections that might contain holdings
        holdings_patterns = [
            r'(?i)held:?\s*(.*?)(?=\.\s*[A-Z]|\Z)',
            r'(?i)conclusion:?\s*(.*?)(?=\.\s*[A-Z]|\Z)',
            r'(?i)judgment:?\s*(.*?)(?=\.\s*[A-Z]|\Z)',
            r'(?i)order:?\s*(.*?)(?=\.\s*[A-Z]|\Z)'
        ]
        
        for pattern in holdings_patterns:
            match = re.search(pattern, text)
            if match and len(match.group(1)) > 50:  # Ensure it's substantial
                return match.group(1).strip()
        
        # If no specific holdings found, return a summary of the last part of the document
        sentences = nltk.sent_tokenize(text)
        if len(sentences) > 5:
            return " ".join(sentences[-5:])
        
        return text[:500] + "..." if len(text) > 500 else text
    
    def _extract_date(self, html_content: str) -> str:
        """
        Extract the date from a case document.
        
        Args:
            html_content: HTML content of the case document
            
        Returns:
            Extracted date in DD MMM YYYY format
        """
        # Clean HTML
        text = self._extract_clean_content(html_content)
        
        # Look for date patterns
        date_patterns = [
            r'(\d{1,2})(?:st|nd|rd|th)?\s+(January|February|March|April|May|June|July|August|September|October|November|December),?\s+(\d{4})',
            r'(\d{1,2})(?:st|nd|rd|th)?\s+(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec),?\s+(\d{4})'
        ]
        
        for pattern in date_patterns:
            match = re.search(pattern, text)
            if match:
                day = match.group(1)
                month = match.group(2)
                year = match.group(3)
                
                # Standardize month abbreviations
                month_map = {
                    'Jan': 'Jan', 'January': 'Jan',
                    'Feb': 'Feb', 'February': 'Feb',
                    'Mar': 'Mar', 'March': 'Mar',
                    'Apr': 'Apr', 'April': 'Apr',
                    'May': 'May',
                    'Jun': 'Jun', 'June': 'Jun',
                    'Jul': 'Jul', 'July': 'Jul',
                    'Aug': 'Aug', 'August': 'Aug',
                    'Sep': 'Sep', 'September': 'Sep',
                    'Oct': 'Oct', 'October': 'Oct',
                    'Nov': 'Nov', 'November': 'Nov',
                    'Dec': 'Dec', 'December': 'Dec'
                }
                
                standardized_month = month_map.get(month, month)
                return f"{day} {standardized_month} {year}"
        
        # If no date found, return empty string
        return ""
    
    def _extract_citation(self, html_content: str) -> str:
        """
        Extract citation from a case document.
        
        Args:
            html_content: HTML content of the case document
            
        Returns:
            Extracted citation
        """
        # Clean HTML
        text = self._extract_clean_content(html_content)
        
        # Look for citation patterns
        citation_patterns = [
            r'(\d{4})\s*AIR\s*(\d+)',
            r'\((\d{4})\)\s*(\d+)\s*SCC\s*(\d+)',
            r'(\d{4})\s*SCR\s*(\d+)'
        ]
        
        for pattern in citation_patterns:
            match = re.search(pattern, text)
            if match:
                if len(match.groups()) == 2:  # AIR or SCR
                    return f"{match.group(1)} {match.group(0).split(match.group(1))[1].strip()}"
                elif len(match.groups()) == 3:  # SCC
                    return f"({match.group(1)}) {match.group(2)} SCC {match.group(3)}"
        
        # If no citation found, return empty string
        return ""
    
    def _generate_analysis(self, brief_text: str, law_sections: List[Dict[str, Any]], 
                         case_histories: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Generate legal analysis based on the brief and search results.
        
        Args:
            brief_text: The brief text
            law_sections: Relevant law sections
            case_histories: Relevant case histories
            
        Returns:
            Dict containing legal analysis
        """
        # Extract key legal issues
        legal_issues = self._extract_legal_issues(brief_text)
        
        # Generate summary
        summary = self._generate_summary(brief_text, law_sections, case_histories, legal_issues)
        
        # Generate arguments
        arguments = self._generate_arguments(brief_text, law_sections, case_histories, legal_issues)
        
        # Generate challenges
        challenges = self._generate_challenges(brief_text, law_sections, case_histories, legal_issues)
        
        # Generate recommendations
        recommendations = self._generate_recommendations(brief_text, law_sections, case_histories, legal_issues)
        
        return {
            "summary": summary,
            "arguments": arguments,
            "challenges": challenges,
            "recommendations": recommendations
        }
    
    def _extract_legal_issues(self, text: str) -> List[str]:
        """
        Extract key legal issues from the brief.
        
        Args:
            text: The brief text
            
        Returns:
            List of legal issues
        """
        # This is a simplified approach - in a real system, this would be more sophisticated
        legal_issues = []
        
        # Look for common legal issue indicators
        issue_indicators = [
            r'(?i)issue(?:s)?\s+(?:is|are|of)?\s*(.*?)(?=\.\s*[A-Z]|\Z)',
            r'(?i)question(?:s)?\s+(?:is|are|of)?\s*(.*?)(?=\.\s*[A-Z]|\Z)',
            r'(?i)matter(?:s)?\s+(?:is|are|of)?\s*(.*?)(?=\.\s*[A-Z]|\Z)',
            r'(?i)dispute(?:s)?\s+(?:is|are|of)?\s*(.*?)(?=\.\s*[A-Z]|\Z)',
            r'(?i)contention(?:s)?\s+(?:is|are|of)?\s*(.*?)(?=\.\s*[A-Z]|\Z)'
        ]
        
        for pattern in issue_indicators:
            matches = re.finditer(pattern, text)
            for match in matches:
                issue = match.group(1).strip()
                if len(issue) > 10:  # Ensure it's substantial
                    legal_issues.append(issue)
        
        # If no issues found using indicators, extract key sentences
        if not legal_issues:
            doc = nlp(text)
            for sent in doc.sents:
                # Look for sentences with legal terms
                if any(term in sent.text.lower() for term in self._extract_legal_terms(text)):
                    if len(sent.text) > 10:
                        legal_issues.append(sent.text)
            
            # Limit to top 3 issues
            legal_issues = legal_issues[:3]
        
        return legal_issues
    
    def _generate_summary(self, brief_text: str, law_sections: List[Dict[str, Any]], 
                        case_histories: List[Dict[str, Any]], legal_issues: List[str]) -> str:
        """
        Generate a summary of the legal analysis.
        
        Args:
            brief_text: The brief text
            law_sections: Relevant law sections
            case_histories: Relevant case histories
            legal_issues: Extracted legal issues
            
        Returns:
            Summary text
        """
        summary = "This case involves "
        
        # Add legal issues
        if legal_issues:
            summary += legal_issues[0]
        else:
            # Extract key legal terms
            legal_terms = self._extract_legal_terms(brief_text)
            if legal_terms:
                summary += f"issues related to {', '.join(legal_terms[:3])}"
            else:
                summary += "various legal issues as described in the brief"
        
        # Add relevant law sections
        if law_sections:
            top_sections = sorted(law_sections, key=lambda x: x['relevance'], reverse=True)[:2]
            section_texts = []
            for section in top_sections:
                section_texts.append(f"{section['title']}, Section {section['sectionNumber']}")
            
            summary += f". The relevant legal provisions include {' and '.join(section_texts)}"
        
        # Add relevant case histories
        if case_histories:
            top_cases = sorted(case_histories, key=lambda x: x['relevance'], reverse=True)[:2]
            case_texts = []
            for case in top_cases:
                case_texts.append(f"{case['parties']} ({case['citation']})")
            
            summary += f". There are precedents from {' and '.join(case_texts)} that may be applicable"
        
        summary += "."
        
        return summary
    
    def _generate_arguments(self, brief_text: str, law_sections: List[Dict[str, Any]], 
                          case_histories: List[Dict[str, Any]], legal_issues: List[str]) -> List[str]:
        """
        Generate potential legal arguments.
        
        Args:
            brief_text: The brief text
            law_sections: Relevant law sections
            case_histories: Relevant case histories
            legal_issues: Extracted legal issues
            
        Returns:
            List of arguments
        """
        arguments = []
        
        # Generate arguments based on law sections
        for section in law_sections[:3]:  # Use top 3 sections
            if section['content']:
                arg = f"Under {section['title']}, Section {section['sectionNumber']}, "
                
                # Extract key phrases from section content
                doc = nlp(section['content'])
                key_phrases = []
                for chunk in doc.noun_chunks:
                    if len(chunk.text.split()) >= 2 and len(chunk.text.split()) <= 5:
                        key_phrases.append(chunk.text)
                
                if key_phrases:
                    arg += f"the elements of {key_phrases[0]} are applicable to this case."
                else:
                    arg += "the legal provisions are applicable to the facts of this case."
                
                arguments.append(arg)
        
        # Generate arguments based on case histories
        for case in case_histories[:3]:  # Use top 3 cases
            if case['holdings']:
                arg = f"The precedent established in {case['parties']} ({case['citation']}) supports the position that "
                
                # Extract key sentence from holdings
                sentences = nltk.sent_tokenize(case['holdings'])
                if sentences:
                    arg += sentences[0]
                else:
                    arg += "similar legal principles apply to the current case."
                
                arguments.append(arg)
        
        # Ensure we have at least 3 arguments
        if len(arguments) < 3:
            # Generate generic arguments based on legal issues
            generic_arguments = [
                "The facts presented in the brief clearly establish the elements required for the legal claim.",
                "The timeline of events demonstrates a clear causal relationship between the actions and the resulting damages.",
                "The documentary evidence supports the legal position outlined in the brief.",
                "The opposing party's actions constitute a clear violation of the applicable legal standards.",
                "The legal precedents consistently support the interpretation of the law as presented in the brief."
            ]
            
            arguments.extend(generic_arguments[:3 - len(arguments)])
        
        return arguments[:5]  # Limit to 5 arguments
    
    def _generate_challenges(self, brief_text: str, law_sections: List[Dict[str, Any]], 
                           case_histories: List[Dict[str, Any]], legal_issues: List[str]) -> List[str]:
        """
        Generate potential legal challenges.
        
        Args:
            brief_text: The brief text
            law_sections: Relevant law sections
            case_histories: Relevant case histories
            legal_issues: Extracted legal issues
            
        Returns:
            List of challenges
        """
        challenges = []
        
        # Generate challenges based on law sections
        for section in law_sections[:2]:  # Use top 2 sections
            challenge = f"The opposing party may argue that {section['title']}, Section {section['sectionNumber']} "
            challenge += "does not apply due to specific factual differences in this case."
            challenges.append(challenge)
        
        # Generate challenges based on case histories
        for case in case_histories[:2]:  # Use top 2 cases
            challenge = f"The precedent in {case['parties']} may be distinguished on grounds that "
            challenge += "the factual circumstances differ significantly from the current case."
            challenges.append(challenge)
        
        # Ensure we have at least 3 challenges
        if len(challenges) < 3:
            # Generate generic challenges
            generic_challenges = [
                "The evidence presented may be insufficient to establish all elements of the legal claim.",
                "There may be procedural hurdles that need to be addressed before the substantive issues can be resolved.",
                "The interpretation of the relevant statutes may be contested by the opposing party.",
                "The causal connection between the alleged actions and the claimed damages may be difficult to establish.",
                "The timeline of events may present challenges in establishing the sequence necessary for the legal claim."
            ]
            
            challenges.extend(generic_challenges[:3 - len(challenges)])
        
        return challenges[:5]  # Limit to 5 challenges
    
    def _generate_recommendations(self, brief_text: str, law_sections: List[Dict[str, Any]], 
                               case_histories: List[Dict[str, Any]], legal_issues: List[str]) -> List[str]:
        """
        Generate legal recommendations.
        
        Args:
            brief_text: The brief text
            law_sections: Relevant law sections
            case_histories: Relevant case histories
            legal_issues: Extracted legal issues
            
        Returns:
            List of recommendations
        """
        recommendations = []
        
        # Generate specific recommendations based on law sections
        if law_sections:
            rec = "Strengthen the legal argument by specifically citing "
            section_texts = []
            for section in law_sections[:2]:
                section_texts.append(f"{section['title']}, Section {section['sectionNumber']}")
            
            rec += f"{' and '.join(section_texts)} and explaining how the facts of the case satisfy the legal requirements."
            recommendations.append(rec)
        
        # Generate specific recommendations based on case histories
        if case_histories:
            rec = "Cite relevant precedents such as "
            case_texts = []
            for case in case_histories[:2]:
                case_texts.append(f"{case['parties']} ({case['citation']})")
            
            rec += f"{' and '.join(case_texts)} to support the legal arguments."
            recommendations.append(rec)
        
        # Add general recommendations
        general_recommendations = [
            "Gather and organize all documentary evidence that supports the factual claims made in the brief.",
            "Consider obtaining expert testimony to strengthen the technical aspects of the case.",
            "Prepare for potential settlement discussions, as many cases are resolved before trial.",
            "Develop counter-arguments to address the potential challenges identified above.",
            "Ensure all procedural requirements are met to avoid unnecessary delays in the legal proceedings."
        ]
        
        recommendations.extend(general_recommendations)
        
        return recommendations[:7]  # Limit to 7 recommendations


# Example usage
if __name__ == "__main__":
    # This is just for testing - in production, the API key should be stored securely
    api_key = "d053cb3e0082a68b58def9f16e1b43c7a497faf4"
    analyzer = LegalBriefAnalyzer(api_key)
    
    # Example brief
    brief = """
    This case involves a breach of contract under Section 73 of the Indian Contract Act. 
    The defendant failed to deliver goods by the agreed date of 15 March 2023, causing significant 
    financial losses to my client. The contract clearly specified a delivery date and included 
    a penalty clause for late delivery. The Supreme Court in Mehta vs. Patel & Others (AIR 2017 SC 567) 
    established that in cases of contractual breach, the aggrieved party must prove actual damages 
    suffered to claim compensation under Section 73 of the Indian Contract Act.
    """
    
    # Analyze the brief
    results = analyzer.analyze_brief(brief)
    print(json.dumps(results, indent=2))
